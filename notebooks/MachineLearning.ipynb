{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9375eee-78bc-4ab5-987c-82d006f9b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREVIS√ÉO DE VENDAS TOTAIS DI√ÅRIAS PARA 2019\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Importar bibliotecas e iniciar Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DailyTotalSalesPrediction\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREVIS√ÉO DE VENDAS TOTAIS DI√ÅRIAS PARA 2019\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18fdad84-955c-4034-9f52-9633b881fef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/6] Carregando dados...\n",
      "Total de pedidos individuais: 102,349\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[1/6] Carregando dados...\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"order_purchase_timestamp\", StringType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"customer_state\", StringType(), True),\n",
    "    StructField(\"latitude_media\", DoubleType(), True),\n",
    "    StructField(\"longitude_media\", DoubleType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_category_name\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"freight_value\", DoubleType(), True),\n",
    "    StructField(\"payment_value\", DoubleType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"precip\", DoubleType(), True),\n",
    "    StructField(\"temp_max\", DoubleType(), True),\n",
    "    StructField(\"temp_min\", DoubleType(), True),\n",
    "    StructField(\"weather\", StringType(), True)\n",
    "])\n",
    "\n",
    "base_path = \"../input\"\n",
    "csv_path = os.path.join(base_path, \"final-dataset\")\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").schema(schema).csv(csv_path)\n",
    "\n",
    "# Converter datas e extrair features temporais\n",
    "df = df.withColumn(\"order_date\", to_date(col(\"order_purchase_timestamp\")))\n",
    "df = df.withColumn(\"year\", year(\"order_date\"))\n",
    "df = df.withColumn(\"month\", month(\"order_date\"))\n",
    "df = df.withColumn(\"dayofweek\", dayofweek(\"order_date\"))\n",
    "\n",
    "# Tratar weather\n",
    "df = df.withColumn(\"weather\", \n",
    "                   when(col(\"weather\") == \"unknown\", \"clear\")\n",
    "                   .when(col(\"weather\").isNull(), \"clear\")\n",
    "                   .otherwise(col(\"weather\")))\n",
    "\n",
    "df = df.fillna({'precip': 0.0, 'temp_max': 25.0, 'temp_min': 18.0})\n",
    "\n",
    "# Features de temperatura\n",
    "df = df.withColumn(\"temp_media\", (col(\"temp_max\") + col(\"temp_min\")) / 2)\n",
    "df = df.withColumn(\"amplitude_termica\", col(\"temp_max\") - col(\"temp_min\"))\n",
    "\n",
    "# Esta√ß√£o do ano\n",
    "df = df.withColumn(\"estacao\", \n",
    "                   when((col(\"month\").between(3, 5)), \"outono\")\n",
    "                   .when((col(\"month\").between(6, 8)), \"inverno\")\n",
    "                   .when((col(\"month\").between(9, 11)), \"primavera\")\n",
    "                   .otherwise(\"verao\"))\n",
    "\n",
    "# Filtrar dados completos\n",
    "df_clean = df.filter(\n",
    "    col(\"payment_value\").isNotNull() & \n",
    "    col(\"price\").isNotNull() &\n",
    "    col(\"freight_value\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"Total de pedidos individuais: {df_clean.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beb87a5d-e74b-4c86-99b1-12ac76145599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/6] Agregando vendas por dia...\n",
      "\n",
      "Total de dias com vendas: 615\n",
      "\n",
      "Exemplo de dados agregados:\n",
      "+----------+------------------+-----------+\n",
      "|order_date|total_vendas_dia  |num_pedidos|\n",
      "+----------+------------------+-----------+\n",
      "|2017-10-06|20840.75          |134        |\n",
      "|2018-04-10|33005.0           |205        |\n",
      "|2017-09-12|32420.91          |206        |\n",
      "|2018-04-25|49824.469999999994|294        |\n",
      "|2017-08-12|19894.08          |103        |\n",
      "+----------+------------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[2/6] Agregando vendas por dia...\")\n",
    "\n",
    "# AGREGAR: soma de vendas + contagem de pedidos por dia\n",
    "df_daily = df_clean.groupBy(\"order_date\", \"year\", \"month\", \"dayofweek\") \\\n",
    "    .agg(\n",
    "        sum(\"payment_value\").alias(\"total_vendas_dia\"),\n",
    "        count(\"order_id\").alias(\"num_pedidos\"),\n",
    "        avg(\"price\").alias(\"avg_price\"),\n",
    "        avg(\"freight_value\").alias(\"avg_freight\"),\n",
    "        avg(\"precip\").alias(\"avg_precip\"),\n",
    "        avg(\"temp_max\").alias(\"avg_temp_max\"),\n",
    "        avg(\"temp_min\").alias(\"avg_temp_min\"),\n",
    "        first(\"estacao\").alias(\"estacao\")\n",
    "    ) \\\n",
    "    .withColumn(\"temp_media\", (col(\"avg_temp_max\") + col(\"avg_temp_min\")) / 2) \\\n",
    "    .withColumn(\"amplitude_termica\", col(\"avg_temp_max\") - col(\"avg_temp_min\"))\n",
    "\n",
    "# Mostrar exemplo\n",
    "print(f\"\\nTotal de dias com vendas: {df_daily.count()}\")\n",
    "print(\"\\nExemplo de dados agregados:\")\n",
    "df_daily.select(\"order_date\", \"total_vendas_dia\", \"num_pedidos\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c74932d0-79b1-4d6d-93a4-47bada11bf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/6] Preparando pipeline de ML...\n",
      "Dias de treino (2016-2017): 373\n",
      "Dias de teste (2018): 242\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[3/6] Preparando pipeline de ML...\")\n",
    "\n",
    "# Indexar esta√ß√£o\n",
    "estacao_indexer = StringIndexer(\n",
    "    inputCol=\"estacao\",\n",
    "    outputCol=\"estacao_index\", \n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Features para prever TOTAL DE VENDAS DO DIA\n",
    "feature_cols = [\n",
    "    'avg_price', 'avg_freight', 'num_pedidos',\n",
    "    'avg_precip', 'avg_temp_max', 'avg_temp_min', \n",
    "    'temp_media', 'amplitude_termica',\n",
    "    'month', 'dayofweek', 'estacao_index'\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[estacao_indexer, assembler])\n",
    "\n",
    "# Split temporal\n",
    "train_df = df_daily.filter(col(\"year\") <= 2017)\n",
    "test_df = df_daily.filter(col(\"year\") == 2018)\n",
    "\n",
    "print(f\"Dias de treino (2016-2017): {train_df.count()}\")\n",
    "print(f\"Dias de teste (2018): {test_df.count()}\")\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)\n",
    "train_data = pipeline_model.transform(train_df)\n",
    "test_data = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae27751f-21c2-46b1-aad7-9d5a5b084a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4/6] Treinando modelos com Cross Validation...\n",
      "  -> Treinando Linear Regression...\n",
      "     RMSE: R$ 3,490.22 | R¬≤: 0.9071\n",
      "  -> Treinando Random Forest...\n",
      "     RMSE: R$ 9,838.81 | R¬≤: 0.2616\n",
      "\n",
      "======================================================================\n",
      "MELHOR MODELO: Linear Regression\n",
      "RMSE: R$ 3,490.22 | R¬≤: 0.9071\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[4/6] Treinando modelos com Cross Validation...\")\n",
    "\n",
    "# --- Linear Regression ---\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"total_vendas_dia\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "lr_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.3]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "lr_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_vendas_dia\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "lr_cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=lr_paramGrid,\n",
    "    evaluator=lr_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "print(\"  -> Treinando Linear Regression...\")\n",
    "lr_model = lr_cv.fit(train_data)\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "lr_rmse = lr_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "lr_r2_eval = RegressionEvaluator(\n",
    "    labelCol=\"total_vendas_dia\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "lr_r2 = lr_r2_eval.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"     RMSE: R$ {lr_rmse:,.2f} | R¬≤: {lr_r2:.4f}\")\n",
    "\n",
    "# --- Random Forest ---\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"total_vendas_dia\", \n",
    "    predictionCol=\"prediction\",\n",
    "    seed=42,\n",
    "    maxBins=50\n",
    ")\n",
    "\n",
    "rf_paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "rf_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_vendas_dia\",\n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "\n",
    "rf_cv = CrossValidator(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=rf_paramGrid,\n",
    "    evaluator=rf_evaluator,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "print(\"  -> Treinando Random Forest...\")\n",
    "rf_model = rf_cv.fit(train_data)\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "rf_rmse = rf_evaluator.evaluate(rf_predictions)\n",
    "\n",
    "rf_r2_eval = RegressionEvaluator(\n",
    "    labelCol=\"total_vendas_dia\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"r2\"\n",
    ")\n",
    "rf_r2 = rf_r2_eval.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"     RMSE: R$ {rf_rmse:,.2f} | R¬≤: {rf_r2:.4f}\")\n",
    "\n",
    "# Selecionar melhor modelo\n",
    "if lr_rmse < rf_rmse:\n",
    "    best_model_name = \"Linear Regression\"\n",
    "    best_model = lr_model\n",
    "    best_rmse = lr_rmse\n",
    "    best_r2 = lr_r2\n",
    "else:\n",
    "    best_model_name = \"Random Forest\"\n",
    "    best_model = rf_model\n",
    "    best_rmse = rf_rmse\n",
    "    best_r2 = rf_r2\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"MELHOR MODELO: {best_model_name}\")\n",
    "print(f\"RMSE: R$ {best_rmse:,.2f} | R¬≤: {best_r2:.4f}\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e7d96c6-794e-4e0b-8039-4e4b87c0c66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5/6] Gerando previs√µes di√°rias para 2019...\n",
      "  -> Criados 313 registros di√°rios\n",
      "  -> Fazendo previs√µes com Linear Regression...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[5/6] Gerando previs√µes di√°rias para 2019...\")\n",
    "\n",
    "# Calcular m√©dias hist√≥ricas por m√™s e dia da semana\n",
    "historical_patterns = df_daily.groupBy(\"month\", \"dayofweek\") \\\n",
    "    .agg(\n",
    "        avg(\"num_pedidos\").alias(\"avg_num_pedidos\"),\n",
    "        avg(\"avg_price\").alias(\"hist_avg_price\"),\n",
    "        avg(\"avg_freight\").alias(\"hist_avg_freight\"),\n",
    "        avg(\"avg_precip\").alias(\"hist_avg_precip\"),\n",
    "        avg(\"avg_temp_max\").alias(\"hist_avg_temp_max\"),\n",
    "        avg(\"avg_temp_min\").alias(\"hist_avg_temp_min\")\n",
    "    ).collect()\n",
    "\n",
    "# Criar dicion√°rio de lookup\n",
    "patterns_dict = {}\n",
    "for row in historical_patterns:\n",
    "    key = (row['month'], row['dayofweek'])\n",
    "    patterns_dict[key] = row\n",
    "\n",
    "# Gerar todas as datas de 2019\n",
    "start_date = datetime(2019, 1, 1)\n",
    "end_date = datetime(2019, 12, 31)\n",
    "predictions_2019 = []\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    month = current_date.month\n",
    "    dayofweek = current_date.isoweekday() + 1  # PySpark: domingo=1\n",
    "    \n",
    "    # Buscar padr√£o hist√≥rico\n",
    "    pattern = patterns_dict.get((month, dayofweek))\n",
    "    \n",
    "    if pattern:\n",
    "        # Esta√ß√£o do ano\n",
    "        if month in [3, 4, 5]:\n",
    "            estacao = \"outono\"\n",
    "        elif month in [6, 7, 8]:\n",
    "            estacao = \"inverno\"\n",
    "        elif month in [9, 10, 11]:\n",
    "            estacao = \"primavera\"\n",
    "        else:\n",
    "            estacao = \"verao\"\n",
    "        \n",
    "        predictions_2019.append({\n",
    "            'order_date': current_date.strftime('%Y-%m-%d'),\n",
    "            'year': 2019,\n",
    "            'month': month,\n",
    "            'dayofweek': dayofweek,\n",
    "            'num_pedidos': pattern['avg_num_pedidos'],\n",
    "            'avg_price': pattern['hist_avg_price'],\n",
    "            'avg_freight': pattern['hist_avg_freight'],\n",
    "            'avg_precip': pattern['hist_avg_precip'],\n",
    "            'avg_temp_max': pattern['hist_avg_temp_max'],\n",
    "            'avg_temp_min': pattern['hist_avg_temp_min'],\n",
    "            'estacao': estacao\n",
    "        })\n",
    "    \n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "print(f\"  -> Criados {len(predictions_2019)} registros di√°rios\")\n",
    "\n",
    "# Converter para DataFrame do Spark\n",
    "df_2019 = spark.createDataFrame(predictions_2019)\n",
    "df_2019 = df_2019.withColumn(\"temp_media\", (col(\"avg_temp_max\") + col(\"avg_temp_min\")) / 2)\n",
    "df_2019 = df_2019.withColumn(\"amplitude_termica\", col(\"avg_temp_max\") - col(\"avg_temp_min\"))\n",
    "\n",
    "# Aplicar transforma√ß√µes\n",
    "df_2019_transformed = pipeline_model.transform(df_2019)\n",
    "\n",
    "# Fazer previs√µes\n",
    "print(f\"  -> Fazendo previs√µes com {best_model_name}...\")\n",
    "df_2019_predictions = best_model.transform(df_2019_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2233adaf-01ba-47dd-a736-c6bc95f1856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[6/6] Exportando resultados...\n",
      "\n",
      "======================================================================\n",
      "RELAT√ìRIO FINAL - PREVIS√ïES DE VENDAS TOTAIS 2019\n",
      "======================================================================\n",
      "\n",
      "üìä MODELO SELECIONADO: Linear Regression\n",
      "   RMSE: R$ 3,490.22\n",
      "   R¬≤: 0.9071\n",
      "\n",
      "üìà TOTAIS HIST√ìRICOS:\n",
      "   2016: R$ 61,080.15\n",
      "   2017: R$ 7,436,454.36\n",
      "   2018: R$ 9,042,746.54\n",
      "\n",
      "üîÆ PREVIS√ÉO 2019:\n",
      "   Total anual: R$ 8,714,976.43\n",
      "   M√©dia di√°ria: R$ 27,843.38\n",
      "   Total de pedidos: 54,618\n",
      "   Dias previstos: 313\n",
      "   Varia√ß√£o vs 2018: -3.62%\n",
      "\n",
      "üíæ ARQUIVO GERADO:\n",
      "   ../output/previsoes_diarias_2019.csv\n",
      "\n",
      "üìã PREVIEW (primeiros 10 dias):\n",
      "      data  vendas_previstas  pedidos_estimados\n",
      "2019-01-01      25540.544988                165\n",
      "2019-01-02      28233.943282                178\n",
      "2019-01-03      21760.172180                149\n",
      "2019-01-04      21995.937913                131\n",
      "2019-01-05      17096.728113                111\n",
      "2019-01-07      23617.707314                155\n",
      "2019-01-08      25540.544988                165\n",
      "2019-01-09      28233.943282                178\n",
      "2019-01-10      21760.172180                149\n",
      "2019-01-11      21995.937913                131\n",
      "\n",
      "üìã ESTAT√çSTICAS MENSAIS:\n",
      "   M√™s 01: R$ 628,514.80\n",
      "   M√™s 02: R$ 576,758.90\n",
      "   M√™s 03: R$ 681,382.04\n",
      "   M√™s 04: R$ 705,631.35\n",
      "   M√™s 05: R$ 763,100.70\n",
      "   M√™s 06: R$ 658,503.89\n",
      "   M√™s 07: R$ 768,907.80\n",
      "   M√™s 08: R$ 804,426.44\n",
      "   M√™s 09: R$ 579,729.89\n",
      "   M√™s 10: R$ 612,226.53\n",
      "   M√™s 11: R$ 1,148,081.62\n",
      "   M√™s 12: R$ 787,712.46\n",
      "\n",
      "======================================================================\n",
      "AN√ÅLISE COMPLETA!\n",
      "======================================================================\n",
      "\n",
      "‚úì Spark encerrado.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n[6/6] Exportando resultados...\")\n",
    "\n",
    "results = df_2019_predictions.select(\n",
    "    \"order_date\", \n",
    "    \"prediction\",\n",
    "    \"num_pedidos\"\n",
    ").orderBy(\"order_date\").collect()\n",
    "\n",
    "# Criar DataFrame pandas\n",
    "predictions_df = pd.DataFrame({\n",
    "    'data': [r['order_date'] for r in results],\n",
    "    'vendas_previstas': [r['prediction'] if r['prediction'] > 0 else 0 for r in results],\n",
    "    'pedidos_estimados': [int(r['num_pedidos']) for r in results]\n",
    "})\n",
    "\n",
    "# Calcular estat√≠sticas\n",
    "total_previsto = predictions_df['vendas_previstas'].sum()\n",
    "media_diaria = predictions_df['vendas_previstas'].mean()\n",
    "total_pedidos = predictions_df['pedidos_estimados'].sum()\n",
    "\n",
    "# Totais hist√≥ricos\n",
    "total_2016 = df_daily.filter(col(\"year\") == 2016).agg(sum(\"total_vendas_dia\")).collect()[0][0] or 0\n",
    "total_2017 = df_daily.filter(col(\"year\") == 2017).agg(sum(\"total_vendas_dia\")).collect()[0][0] or 0\n",
    "total_2018 = df_daily.filter(col(\"year\") == 2018).agg(sum(\"total_vendas_dia\")).collect()[0][0] or 0\n",
    "\n",
    "# Salvar CSV\n",
    "output_path = \"../output/previsoes_diarias_2019.csv\"\n",
    "os.makedirs(\"../output\", exist_ok=True)\n",
    "predictions_df.to_csv(output_path, index=False)\n",
    "\n",
    "# ============================================================================\n",
    "# RELAT√ìRIO FINAL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RELAT√ìRIO FINAL - PREVIS√ïES DE VENDAS TOTAIS 2019\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä MODELO SELECIONADO: {best_model_name}\")\n",
    "print(f\"   RMSE: R$ {best_rmse:,.2f}\")\n",
    "print(f\"   R¬≤: {best_r2:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà TOTAIS HIST√ìRICOS:\")\n",
    "print(f\"   2016: R$ {total_2016:,.2f}\")\n",
    "print(f\"   2017: R$ {total_2017:,.2f}\")\n",
    "print(f\"   2018: R$ {total_2018:,.2f}\")\n",
    "\n",
    "print(f\"\\nüîÆ PREVIS√ÉO 2019:\")\n",
    "print(f\"   Total anual: R$ {total_previsto:,.2f}\")\n",
    "print(f\"   M√©dia di√°ria: R$ {media_diaria:,.2f}\")\n",
    "print(f\"   Total de pedidos: {total_pedidos:,}\")\n",
    "print(f\"   Dias previstos: {len(predictions_df)}\")\n",
    "\n",
    "if total_2018 > 0:\n",
    "    variacao = ((total_previsto - total_2018) / total_2018) * 100\n",
    "    print(f\"   Varia√ß√£o vs 2018: {variacao:+.2f}%\")\n",
    "\n",
    "print(f\"\\nüíæ ARQUIVO GERADO:\")\n",
    "print(f\"   {output_path}\")\n",
    "\n",
    "print(\"\\nüìã PREVIEW (primeiros 10 dias):\")\n",
    "print(predictions_df.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\nüìã ESTAT√çSTICAS MENSAIS:\")\n",
    "predictions_df['mes'] = pd.to_datetime(predictions_df['data']).dt.month\n",
    "monthly_stats = predictions_df.groupby('mes')['vendas_previstas'].sum()\n",
    "for mes, total in monthly_stats.items():\n",
    "    print(f\"   M√™s {mes:02d}: R$ {total:,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AN√ÅLISE COMPLETA!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\n‚úì Spark encerrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaa3291-2157-4ea2-9008-2da6a72ef827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ETL Env)",
   "language": "python",
   "name": "etl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
